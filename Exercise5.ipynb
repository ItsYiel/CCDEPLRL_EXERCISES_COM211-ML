{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOyWxUjvo2d9nW7EQPtFMwN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marielollage/CCDEPLRL_EXERCISES_COM211-ML/blob/main/Exercise5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sonnets.txt \\\n",
        "    -O /tmp/sonnets.txt\n",
        "data = open('/tmp/sonnets.txt').read()\n",
        "\n",
        "corpus = data.lower().split(\"\\n\")\n",
        "\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Define max_sequence_len\n",
        "max_sequence_len = 20\n",
        "\n",
        "# Check if input_sequences is empty\n",
        "if len(input_sequences) == 0:\n",
        "    print(\"Error: Input sequences are empty. Check the data preprocessing step.\")\n",
        "else:\n",
        "    # Convert input_sequences to a NumPy array\n",
        "    input_sequences = np.array(input_sequences)\n",
        "\n",
        "    # Define predictors and label\n",
        "    predictors = input_sequences[:, :-1]\n",
        "    label = input_sequences[:, -1]\n",
        "\n",
        "    # One-hot encode the labels\n",
        "    label = tf.keras.utils.to_categorical(label, num_classes=total_words)\n",
        "\n",
        "    # Check the shapes of predictors and label\n",
        "    print(\"Shape of predictors:\", predictors.shape)\n",
        "    print(\"Shape of label:\", label.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGJZ68VsT2Q3",
        "outputId": "a9fec848-82ce-419e-bdb7-7e9f1d06a927"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-01 05:44:47--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sonnets.txt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.170.207, 142.251.175.207, 74.125.24.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.170.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2024-06-01 05:44:48 ERROR 404: Not Found.\n",
            "\n",
            "Error: Input sequences are empty. Check the data preprocessing step.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(150)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "# Pick an optimizer\n",
        "adam = Adam(lr=0.01)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "\n",
        "# Check if predictors and label are empty\n",
        "if len(predictors) == 0 or len(label) == 0:\n",
        "    print(\"Error: Empty data. Check data preprocessing step.\")\n",
        "else:\n",
        "    # Train the model\n",
        "    history = model.fit(predictors, label, epochs=100, verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxw_Io6kT_-b",
        "outputId": "6128e151-793c-4549-d7a7-5c6ea85201e9"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_32\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_29 (Embedding)    (None, 19, 100)           100       \n",
            "                                                                 \n",
            " bidirectional_29 (Bidirect  (None, 300)               301200    \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 1)                 301       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 301601 (1.15 MB)\n",
            "Trainable params: 301601 (1.15 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Error: Empty data. Check data preprocessing step.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = None\n",
        "try:\n",
        "    history = model.fit(predictors, label, epochs=100, verbose=1)\n",
        "except ValueError as e:\n",
        "    print(\"Error during model training:\", e)\n",
        "\n",
        "# Plot training history if training was successful\n",
        "if history:\n",
        "    acc = history.history['accuracy']\n",
        "    loss = history.history['loss']\n",
        "\n",
        "    epochs = range(len(acc))\n",
        "\n",
        "    plt.plot(epochs, acc, 'b', label='Training accuracy')\n",
        "    plt.title('Training accuracy')\n",
        "\n",
        "    plt.figure()\n",
        "\n",
        "    plt.plot(epochs, loss, 'b', label='Training Loss')\n",
        "    plt.title('Training loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Training failed. Cannot plot training history.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sG5NsqY_Ue54",
        "outputId": "90bbda19-9368-4e65-951d-8fa01c8a30da"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error during model training: Expected input data to be non-empty.\n",
            "Training failed. Cannot plot training history.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text = \"Help me Obi Wan Kenobi, you're my only hope\"\n",
        "next_words = 100\n",
        "\n",
        "for _ in range(next_words):\n",
        "    # Tokenize the seed text\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    # Pad the token list\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "    # Predict the probabilities for each word\n",
        "    predicted_probs = model.predict(token_list, verbose=0)[0]\n",
        "    # Get the index of the word with the highest probability\n",
        "    predicted_index = np.argmax(predicted_probs)\n",
        "    output_word = \"\"\n",
        "    # Find the predicted word based on the index\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted_index:\n",
        "            output_word = word\n",
        "            break\n",
        "    # Append the predicted word to the seed text\n",
        "    seed_text += \" \" + output_word\n",
        "\n",
        "print(seed_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFq1MpVTT4uX",
        "outputId": "e921e539-5f11-4a45-8371-dac1a8375475"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help me Obi Wan Kenobi, you're my only hope                                                                                                    \n"
          ]
        }
      ]
    }
  ]
}